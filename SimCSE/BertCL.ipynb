{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BertCL.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "!pip install seaborn"
   ],
   "metadata": {
    "id": "guuJvKirXiQr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "af4a9803-1f0d-4269-9056-199751037ec5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
      "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib>=2.2->seaborn) (4.2.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from dataclasses import field, dataclass\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "import seaborn\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ],
   "metadata": {
    "id": "bp1PeoKFT3jW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Definiamo un dataset per il contrastive learning con text e positive pair"
   ],
   "metadata": {
    "id": "J0WSSINlW-qa",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class CLDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.pairs = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.pairs.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.pairs['text'].iloc[idx]\n",
    "        positive = self.pairs['positive'].iloc[idx]\n",
    "        negative = self.pairs['negative'].iloc[idx]\n",
    "        ade = self.pairs['ade'].iloc[idx]\n",
    "        meddra = self.pairs['meddra'].iloc[idx]\n",
    "        negative_ade = self.pairs['negative_ade'].iloc[idx]\n",
    "        negative_meddra = self.pairs['negative_meddra'].iloc[idx]\n",
    "        positive_ade = self.pairs['positive_ade'].iloc[idx]\n",
    "        positive_meddra = self.pairs['positive_meddra'].iloc[idx]                \n",
    "        return text, positive, negative, ade, meddra, positive_ade, positive_meddra, negative_ade, negative_meddra\n"
   ],
   "metadata": {
    "id": "Co_zQgeQWicJ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creiamo il modello usando BERT per il calcolo degli embedding e un layer lineare che allontanerà e avvicinerà gli embedding"
   ],
   "metadata": {
    "id": "tGybxGvCXGnr",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "EXlcxLnIWPg4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class BertCL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BertCL, self).__init__()\n",
    "\n",
    "        self.checkpoint = 'dmis-lab/biobert-v1.1'\n",
    "\n",
    "        # Bert per andare da token a embedding\n",
    "        self.bert_config = AutoConfig.from_pretrained(self.checkpoint)\n",
    "        self.bert = AutoModel.from_pretrained(self.checkpoint)\n",
    "\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # NN per effettuare il contrastive learning\n",
    "        self.l1 = nn.Linear(self.bert_config.hidden_size, self.bert_config.hidden_size)\n",
    "\n",
    "        self.drop = nn.Dropout(p=0.4)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, tokenized_sentences):\n",
    "        sentence_embeddings = self.bert(**tokenized_sentences)\n",
    "        result = sentence_embeddings.last_hidden_state[:, 0, :]\n",
    "        result = self.l1(result)\n",
    "        return self.activation(result)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train = pd.read_csv('/content/positive_negative_pairs_train.csv')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "TRAINING_EPOCHS = 4\n",
    "\n",
    "checkpoint = 'dmis-lab/biobert-v1.1'\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "bert_config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "model = BertCL()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "train_dataset = CLDataset(train)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                              num_workers=2,\n",
    "                              pin_memory=True, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class ContrastiveTrainingLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(ContrastiveTrainingLoss, self).__init__()\n",
    "        self.cos_sim = torch.nn.CosineSimilarity() \n",
    "\n",
    "    def forward(self, inputs, positives, negatives):\n",
    "        batch_size = inputs.shape[0]\n",
    "        min_same_pair = self.cos_sim(inputs, positives).abs().mean()\n",
    "        max_diff_pair = self.cos_sim(inputs, negatives).abs().mean()\n",
    "        \n",
    "        return F.relu(min_same_pair + max_diff_pair)"
   ],
   "metadata": {
    "id": "O5LVoKbZWiuL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training e testing"
   ],
   "metadata": {
    "id": "NDKr7WpZXuVI",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "loss = ContrastiveTrainingLoss()\n",
    "running_loss = []\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(TRAINING_EPOCHS), disable=False):\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "        sentences, positives, negatives, ades, meddras, positive_ades, positive_meddras, negative_ades, negative_meddras = batch\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        tokenization_sentences = bert_tokenizer(list(sentences), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\")\n",
    "        tokenization_positives = bert_tokenizer(list(positives), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\") \n",
    "        tokenization_negatives = bert_tokenizer(list(negatives), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\") \n",
    "        tokenization_ades = bert_tokenizer(list(ades), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\")\n",
    "        tokenization_meddras = bert_tokenizer(list(meddras), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\")\n",
    "        tokenization_positive_ades = bert_tokenizer(list(positive_ades), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\")\n",
    "        tokenization_negative_ades = bert_tokenizer(list(negative_ades), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\")\n",
    "        tokenization_negative_meddras = bert_tokenizer(list(negative_meddras), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\")\n",
    "\n",
    "        tokenization_sentences.to(device)\n",
    "        tokenization_positives.to(device)\n",
    "        tokenization_negatives.to(device)\n",
    "        tokenization_ades.to(device)\n",
    "        tokenization_meddras.to(device)\n",
    "        tokenization_positive_ades.to(device)\n",
    "        tokenization_negative_ades.to(device)\n",
    "        tokenization_negative_meddras.to(device)\n",
    "\n",
    "        sentence_output = model(tokenization_sentences)\n",
    "        positive_output = model(tokenization_positives)\n",
    "        negative_output = model(tokenization_negatives)\n",
    "        ades_output = model(tokenization_ades)\n",
    "        meddra_output = model(tokenization_meddras)\n",
    "        positive_ade_output = model(tokenization_positive_ades)\n",
    "        negative_meddra_output = model(tokenization_negative_meddras)\n",
    "        negative_ade_output = model(tokenization_negative_ades)\n",
    "\n",
    "        # Avviciniamo i positive pairs e i negative pairs sia nel caso delle frasi \n",
    "        # sia nel caso degli ade\n",
    "        sentences_loss = loss(sentence_output, positive_output, negative_output)\n",
    "        output1 = loss(ades_output, positive_ade_output, negative_ade_output)\n",
    "        # Avviciniamo gli ade ai corrispondenti meddra allontanando gli altri ade\n",
    "        output2 = loss(ades_output, meddra_output, negative_meddra_output)\n",
    "        output3 = loss(meddra_output, meddra_output, negative_meddra_output)\n",
    "        \n",
    "        output = sentences_loss + output1 + output2 + output3\n",
    "\n",
    "        output.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(output.item())\n",
    "\n",
    "seaborn.lineplot(x=range(len(running_loss)), y=running_loss)"
   ],
   "metadata": {
    "id": "P-wqnTpTYE8G",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "outputId": "03242591-22b6-4666-ee3b-64bf55ea4500",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 4/4 [22:27<00:00, 336.86s/it]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f38aa594d10>"
      ]
     },
     "metadata": {},
     "execution_count": 9
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class CLTestDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data['text'].iloc[idx]\n",
    "        ade = self.data['ade'].iloc[idx]\n",
    "        meddra = self.data['meddra'].iloc[idx]\n",
    "            \n",
    "        return text, ade, meddra\n",
    "\n",
    "test = pd.read_csv('/content/positive_negative_pairs_test.csv')\n",
    "test_dataset = CLTestDataset(test)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "to_classify = test.shape[0]\n",
    "correctly_classified = 0\n",
    "correct_meddra_in_top_5 = 0\n",
    "wrongly_classified = 0\n",
    "\n",
    "# codifichiamo i meddra \n",
    "unique_meddras = test['meddra'].unique()\n",
    "tokenization_meddra = bert_tokenizer(list(unique_meddras), padding=True, truncation=True,\n",
    "                                     max_length=bert_config.max_position_embeddings,\n",
    "                                     return_tensors=\"pt\")\n",
    "tokenization_meddra.to(device)\n",
    "encoded_meddras = model(tokenization_meddra)\n",
    "encoded_meddras.to(device)\n",
    "\n",
    "for batch_idx, batch in enumerate(tqdm(test_dataloader)):\n",
    "    sentences, ades, meddras = batch\n",
    "    \n",
    "    tokenization_ades = bert_tokenizer(list(ades), padding=True, truncation=True,\n",
    "                                                max_length=bert_config.max_position_embeddings,\n",
    "                                                return_tensors=\"pt\")\n",
    "    tokenization_ades.to(device)\n",
    "\n",
    "    output = model(tokenization_ades)\n",
    "\n",
    "    # Controlliamo quale dei meddra ha l'embedding più vicino al ade\n",
    "    # Ogni riga è la distanza dall'ade in riga i al meddra in colonna j\n",
    "    distance_encoding_to_meddra = torch.cdist(output, encoded_meddras, p=2)\n",
    "\n",
    "    # Per ogni riga prendiamo l'indice dei 5 meddra più vicini all'ade\n",
    "    _, closest_meddra_to_ade_indices = torch.topk(distance_encoding_to_meddra, k=5, dim=1, largest=False)\n",
    "\n",
    "    for (meddra, closest_meddras_to_ade) in zip(list(meddras), closest_meddra_to_ade_indices):\n",
    "      closest_meddras = [ unique_meddras[index] for index in closest_meddras_to_ade]\n",
    "\n",
    "      if closest_meddras[0] == meddra:\n",
    "        correctly_classified += 1\n",
    "      elif meddra in closest_meddras:\n",
    "        correct_meddra_in_top_5 += 1\n",
    "      else:\n",
    "        wrongly_classified += 1\n",
    "\n",
    "print()\n",
    "print(f\"Examples: {to_classify}\")\n",
    "print(f\"Correctly classified: {correctly_classified}\")\n",
    "print(f\"Correct meddra is in the top 5: {correct_meddra_in_top_5}\")\n"
   ],
   "metadata": {
    "id": "Db9fWa4Iy66T",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0028b4b9-c1d7-435c-df13-8d5c5d58686a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 23/23 [00:08<00:00,  2.78it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Examples: 5864\n",
      "Correctly classified: 1598\n",
      "Correct meddra is in the top 5: 439\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(test[test['ade'] == test['meddra']]))"
   ],
   "metadata": {
    "id": "kiemuX8Muk_v",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "vis = test.sample(n=1000, random_state=7)\n",
    "\n",
    "ade_tok =  bert_tokenizer(vis['ade'].tolist(), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\")\n",
    "meddra_tok = bert_tokenizer(vis['meddra'].unique().tolist(), padding=True, truncation=True, max_length=bert_config.max_position_embeddings, return_tensors=\"pt\")\n",
    "ade_tok.to(device)\n",
    "meddra_tok.to(device)\n",
    "\n",
    "model.eval()\n",
    "ade_emb = model(ade_tok).tolist()\n",
    "meddra_emb = model(meddra_tok).tolist()\n",
    "\n",
    "del ade_tok\n",
    "del meddra_tok\n",
    "\n",
    "ade_coords = TSNE(n_components=2, random_state=7).fit_transform(ade_emb)\n",
    "meddra_coords = TSNE(n_components=2, random_state=7).fit_transform(meddra_emb)\n",
    "\n",
    "x = [el[0] for el in ade_coords]\n",
    "y = [el[1] for el in ade_coords]\n",
    "\n",
    "md_x = [el[0] for el in meddra_coords]\n",
    "md_y = [el[1] for el in meddra_coords]\n",
    "\n",
    "vis_ade = pd.DataFrame({\n",
    "    'type': 'ade',\n",
    "    'meddra': vis['meddra'],\n",
    "    'x' : x,\n",
    "    'y' : y,\n",
    "})\n",
    "\n",
    "vis_meddra = pd.DataFrame({\n",
    "    'type': 'meddra',\n",
    "    'meddra': vis['meddra'].unique(),\n",
    "    'x' : md_x,\n",
    "    'y' : md_y\n",
    "})\n",
    "\n",
    "vis_df = pd.concat([vis_ade, vis_meddra])\n",
    "\n",
    "sns.scatterplot(data=vis_df, \n",
    "                x='x', y='y', \n",
    "                hue='meddra', \n",
    "                style='type').legend([],[], frameon=False)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "id": "0wa89v5bnlZF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}